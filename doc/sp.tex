\class{Neighborhood} classes do not implement any strategy of which neighborhood operation to choose. 
Search procedures are using a \class{Neighborhood} and define this strategy. The classes implemented 
are \class{FirstImprovement}, \class{BestImprovement}, \class{TabuSearch}, and \class{RandomWalk}. \\
\class{FirstImprovement}, \class{BestImprovement}, and \class{RandomWalk} are implementation of local search 
algorithms of almost same name and can be used together with any of the \class{Neighborhood} classes. 
\class{TabuSearch} is an implementation of the metaheuristic tabu search using a tabu tenure, and 
an aspiration criteria. \medskip \\
The class \class{BestImprovement} looks at each \class{Move} a \class{Neighborhood} class $NE$ gives and finds uses the 
\class{Move} that leads to the best solution. The best \class{Move} is determined from their delta vector after the 
method \method{calculateDelta()} of the \class{Neighborhood} is called on each \class{Move} returned by the 
neighborhood until \textbf{NULL} is returned. \class{BestImprovement} returns a flag that tells if the current 
solution was improved. A flag can be given to \class{BestImprovement} that indicate if it should commit a non 
improving \class{Move}. How each iteration is done is describe by algorithm \ref{algo_BI} \\ 
\IncMargin{1em}
\begin{algorithm}[H]

%\SetKwFunction{relation}{relation}\SetKwFunction{coeff}{coefficient}
\SetKwFunction{makeOneway}{makeOneway}
\SetKwFunction{defi}{defines}
\algdata
\Input{\bool alwaysCommit, \class{Neighborhood} NE}
\Output{\bool improvement}
\BlankLine
%\If{c \upshape already defines a oneway constraint}{\Return{} \false\; \boste{This constraint could be removed in 
%$O(\alpha(c_j))$}}
% \If{\upshape Number of integer variables not defined $> 1$}{\Return{} \false\; \boste{Needed in order to create the 
% right update queue}}
%\If{$|Y(c)| > 1$}{\Return{} \false}
%\If{\relation{c} \upshape is (==) }{\Return{} \true}
%\If{$c_j$ \upshape is not a linear equality}{
%  \Return{} \false}
%\int coeff = \coeff{c,v}\;
%\upshape in $\bigcup\limits_{o \in O} f_o(\vec{v})$}{	
%\tcp{Find the best variable to define}
\class{Move} bestMove = NE.next() \;
\class{Move} move = NE.next() \;
\While{move $\neq$ NULL}{
  \bool allowed = NE.calculateDelta(move) \;
  \If{!allowed}{
    move = NE.next() \;
    \continue \;
  }
  bestMove = compareMove(move,bestMove) \;  
  move = NE.next() \;
}
\bool improvement = Check if bestMove gives improvement \;
\tcp{by looking at delta vector} \;
\If{improvement \textbf{OR} alwaysCommit}{
  NE.commitMove(bestMove)
 }
\Return improvement \;

\caption{BestImprovement Start} \label{algo_BI} 
 %\caption{Test if a constraint $c$ can define a variable $x$ } \label{algo_checkoneway}
\end{algorithm} \noindent
\DecMargin{1em} \\
If \class{BestImprovement} is combined with the neighborhood class \class{RandomConflictConNE} it gives a minimim 
conflict heuristic that can be useful to reach a feasible solution. \\ 
\class{FirstImprovement} has an implementation very similar to \class{BestImprovement}. Instead of calculating each 
\class{Move} of a \class{Neighborhood} class $NE$ it stops requesting a \class{Move} once an improving \class{Move} is 
found. If no improving \class{Move} is found, when $NE$ returns a \textbf{NULL} pointer it does not commit a 
\class{Move}. If no improving \class{Move} is found the current solution is in a local optima with the regard to the 
chosen \class{Neighborhood} class. \\ 
The class \class{RandomWalk} uses the method \method{nextRandom()} from its \class{Neighborhood} $NE$ and if that 
\class{Move} is allowed it is committed. It takes an integer as argument that indicate the number of times it is 
repeated. The benefit is many iteration can be done but they are not as likely to have a good quality.  
Though tabu search is a metaheuristic it is implemented the same way as the other search procedures but with some 
additions. It takes four arguments; the number of iterations made so far, the best solution found, the current 
solution, and a tabu list. The implementation is similar to \class{BestImprovement} with additional checks with regard 
to the tabu list and aspiration criteria. The aspiration criteria is if a neighborhood operation is tabu but leads to a 
solution better than one found so far, the tabu list is ignored and that neighborhood operation is done. The 
tabu list has size $n$ and each variable has a integer corresponding to the last iteration that variable was 
changed. The tabu tenure is chosen to be based on the neighborhood size with a small degree of random. $tt = 
random(0,10) + min(2\cdot neigborhood size, n/200)$. The neighborhood size might be very small when only considering 
variables that are in a conflicting constraint, hence the multiplier. \\ 
The algorithm for tabu search for a single flip neighborhood sketched by algorithm \ref{algo_TS}. \\ 
\IncMargin{1em}
\begin{algorithm}[H]

%\SetKwFunction{relation}{relation}\SetKwFunction{coeff}{coefficient}
  \algdata
\Input{\int iteration, int[] best, int[] current, int[] tabulist }
%\Output{\bool improvement}
\BlankLine
\int tabuTenure = Random(0,10)+ min(NE.getSize()*2, tabulist.size() /200) \;
\class{Move} bestMove = NE.next() \;
\class{Move} move = NE.next() \;
\While{move != NULL}{
  \bool allowed = NE.calculateDelta(move) \;
  \If{!allowed}{
    move = NE.next() \;
    \continue \;
  }
  \bool isTabu = (iteration - tabulist[move.ID]) $<=$ tabutenure \;
  \If{isTabu}{
    \If{betterThanBest(current,move.getDeltaVector(), best)}{
      NE.commitMove(move)
      tabulist[move.ID] = iteration \;
      \Return \true \;
      
    }{
      move = NE.next() \;
      \continue \;
    }  
  }
  bestMove = compareMove(move,bestMove) \;  
  move = NE.next() \;
}
%\bool improvement = Check if bestMove gives improvement to current solution 
%\tcp{by looking at delta vector} \;
NE.commitMove(bestMove) \;
%\Return improvement \;

\caption{TabuSearch Start(iteration, best,current,tabulist)} \label{algo_TS} 
 %\caption{Test if a constraint $c$ can define a variable $x$ } \label{algo_checkoneway}
\end{algorithm} \noindent
\DecMargin{1em} \\
\class{TabuSearch} needs to be combined with a \class{Neighborhood} class that uses single flip neighborhood operation. 
This makes it less flexible in combining it with a \class{Neighborhood} class than the other search procedures 
\class{FirstImprovement}, \class{BestImprovement}, and \class{RandomWalk}. \\ 
In order to create an efficient local search we need to change search procedure at some point, with the exception of 
tabu search that can perform well on it own. 



