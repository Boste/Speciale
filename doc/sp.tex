\class{Neighborhood} classes do not implement any strategy of which neighborhood operation that should be chosen. 
Search procedures are using a neighborhood and define this strategy. The classes implemented 
are \class{FirstImprovement}, \class{BestImprovement}, \class{TabuSearch}, and \class{RandomWalk}. 
\class{FirstImprovement}, \\ \class{BestImprovement}, and \class{RandomWalk} are implementation of local search 
algorithms of almost same name and can be used together with any of the \class{Neighborhood} classes. 
\class{TabuSearch} is an implementation of the metaheuristic tabu search using best improvement, a tabu tenure, and 
an aspiration criteria. \medskip \\
The class \class{BestImprovement} looks at each \class{Move} a \class{Neighborhood} class $NE$ gives and finds the 
best \class{Move}. The best \class{Move} is determined from their delta vector after the method \method{claculatDelta} 
of the \class{Neighborhood} is called on each \class{Move}. It returns a boolean that tells if the current solution was 
improved. A boolean can be given to \class{BestImprovement} that indicate if it should commit a non improving 
\class{Move}. How each iteration is done is describe by algorithm \ref{algo_BI} \\ 
\boste{Skriv best improvement alg, min conflict, muligvis tabu 
 Søndag} \\
\boste{Linear, sum, LEQ og EQ start søndag. Færdig mandag + ret}

\IncMargin{1em}
\begin{algorithm}[H]

%\SetKwFunction{relation}{relation}\SetKwFunction{coeff}{coefficient}
\SetKwFunction{makeOneway}{makeOneway}
\SetKwFunction{defi}{defines}
\algdata
\Input{\bool alwaysCommit}
\Output{\bool improvement}
\BlankLine
%\If{c \upshape already defines a oneway constraint}{\Return{} \false\; \boste{This constraint could be removed in 
%$O(\alpha(c_j))$}}
% \If{\upshape Number of integer variables not defined $> 1$}{\Return{} \false\; \boste{Needed in order to create the 
% right update queue}}
%\If{$|Y(c)| > 1$}{\Return{} \false}
%\If{\relation{c} \upshape is (==) }{\Return{} \true}
%\If{$c_j$ \upshape is not a linear equality}{
%  \Return{} \false}
%\int coeff = \coeff{c,v}\;
%\upshape in $\bigcup\limits_{o \in O} f_o(\vec{v})$}{	
%\tcp{Find the best variable to define}
\class{Move} bestMove = NE.next() \;
\class{Move} move = NE.next() \;
\While{move != NULL}{
  \bool allowed = NE.calculateDelta(move) \;
  \If{!allowed}{
    move = NE.next() \;
    \continue \;
  }
  bestMove = compareMove(move,bestMove) \;  
  move = NE.next() \;
}
\bool improvement = Check if bestMove gives improvement \;
\tcp{by looking at delta vector} \;
\If{improvement or alwaysCommit}{
  NE.commitMove(bestMove)
 }
\Return improvement \;

\caption{BestImprovement Start} \label{algo_BI} 
 %\caption{Test if a constraint $c$ can define a variable $x$ } \label{algo_checkoneway}
\end{algorithm} \noindent
\DecMargin{1em} \\
If \class{BestImprovement} is combined with the neighborhood class \class{RandomConflictConNE} it gives a minimim 
conflict heuristic that can be useful to reach a feasible solution. \\ 
\class{FirstImprovement} has a very similar implementation. Instead of calculating each \class{Move} of a 
\class{Neighborhood} class $NE$ it stops requesting a \class{Move} once an improving \class{Move} is found. If no 
improving \class{Move} is found when $NE$ returns a \textbf{NULL} pointer it does not commit a \class{Move}. If no 
improving \class{Move} is found the current solution is in a local optima with the regard to the chosen 
\class{Neighborhood}. \\ 
The class \class{RandomWalk} uses the method \method{nextRandom()} from its \class{Neighborhood} $NE$ and if that 
\class{Move} is allowed it is committed. It takes an integer as argument that indicate the number of times it is 
repeated. The benefits is create new solution very fast but they are not likely to have a good quality.  
Though tabu search is a metaheuristic it is implemented the same way as the other search procedures but with some 
additions. It takes four arguments; the number of steps made(iterations), the best solution found, the current 
solution, and a tabu list. The implementation is similar to \class{BestImprovement} with additional checks with regard 
to the tabu list and aspiration criteria. If a neighborhood operation is tabu but leads to a solution better than one 
found so far, the tabu list is ignored. The algorithm for tabu search for a single flip neighborhood sketched by 
algorithm \ref{algo_TS}. \\ 
\IncMargin{1em}
\begin{algorithm}[H]

%\SetKwFunction{relation}{relation}\SetKwFunction{coeff}{coefficient}
  \algdata
\Input{\int iteration, int[] best, int[] current, vector<int> tabulist }
%\Output{\bool improvement}
\BlankLine
\int tabuTenure = Random(0,10)+ min(NE.getSize()*2, tabulist.size() /200) \;
\class{Move} bestMove = NE.next() \;
\class{Move} move = NE.next() \;
\While{move != NULL}{
  \bool allowed = NE.calculateDelta(move) \;
  \If{!allowed}{
    move = NE.next() \;
    \continue \;
  }
  \bool isTabu = (iteration - tabulist[move.ID]) $<=$ tabutenure \;
  \If{isTabu}{
    \If{betterThanBest(current,move.getDeltaVector(), best)}{
      NE.commitMove(move)
      tabulist[move.ID] = iteration \;
      \Return \true \;
      
    }{
      move = NE.next() \;
      \continue \;
    }  
  }
  bestMove = compareMove(move,bestMove) \;  
  move = NE.next() \;
}
%\bool improvement = Check if bestMove gives improvement to current solution 
%\tcp{by looking at delta vector} \;
NE.commitMove(bestMove) \;
%\Return improvement \;

\caption{TabuSearch Start} \label{algo_TS} 
 %\caption{Test if a constraint $c$ can define a variable $x$ } \label{algo_checkoneway}
\end{algorithm} \noindent
\DecMargin{1em} \\
\class{TabuSearch} needs to be combined with a \class{Neighborhood} class that uses single flip neighborhood operation. 
This makes it less flexible in combining it with a \class{Neighborhood} class than the other search procedures 
\class{FirstImprovement}, \class{BestImprovement}, and \class{RandomWalk}. \\ 
In order to create an efficient local search we need to change search procedure at some point, with the exception of 
tabu search that can perform well on it own. 



